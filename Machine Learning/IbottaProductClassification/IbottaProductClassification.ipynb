{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e98fc6",
   "metadata": {},
   "source": [
    "This project deals with classifying products in a dataset from Ibotta. Products are classified as one of 7 different categories using the name of the product and the product brand. This is done using a tokenizer to build a dictionary of vectors associated with words commonly found in the product names and brands.\n",
    "\n",
    "I tried several different types of models to classify the Ibotta dataset. First I explored using CNNs without recursion, varying the pooling structure, the number of convolutional layers, and the kernel sizes of these layers. Then I tried using RNNs without convolution. I tried using simple RNN layers, LSTM layers, and GRU layers, all while varying the number of layers and the number of parameters in each layer. For both the CNN and RNN models, I also testing various dropout and regularization rates. However, none of these models had as much predictive power as the models that combined recursion and convolution.\n",
    " \n",
    "My final model is a recurrent convolutional neural network. The data is first run through a time distributed CNN with two convolutional layers and a dense layer. One convolutional layer has a smaller kernal size than the other, but all three layers utilize batch normalization and L2 regularization. There is a dropout layer after the time distributed CNN, followed by a bidirectional GRU layer with recurrent dropout and batch normalization. Then there is another dense layer with dropout, batch normalization, and L2 regularization. Finally, there is a dense layer with a softmax activation to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9b5da1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in various libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from random import seed\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Embedding, GRU,\\\n",
    "    Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D,\\\n",
    "    Flatten, TimeDistributed, Dropout, BatchNormalization, SimpleRNN\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.utils import to_categorical as to_cat\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5d9f5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in and format data\n",
    "train = pd.read_csv('ibotta_train.csv')\n",
    "test = pd.read_csv('ibotta_test.csv')\n",
    "train_name = train['Name']\n",
    "train_brand = train['Brand_name']\n",
    "test_id = test['Id']\n",
    "test_name = test['Name']\n",
    "test_brand = test['Brand_name']\n",
    "train_labels = train['Cat_code']\n",
    "#combine data for processing\n",
    "names = pd.concat([train_name, test_name]).reset_index(drop=True)\n",
    "brands = pd.concat([train_brand, test_brand]).reset_index(drop=True)\n",
    "brands = brands.fillna('unknown')\n",
    "data = names + ' ' + brands\n",
    "#convert strings to integer sequences\n",
    "max_words = 5000\n",
    "max_len = 15\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "word_index = tokenizer.word_index\n",
    "#pad sequences\n",
    "data = pad_sequences(sequences, maxlen = max_len)\n",
    "#split data back into training and testing sets\n",
    "train = data[:8000]\n",
    "test = data[8000:]\n",
    "#convert labels to categorical data\n",
    "train_labels = to_cat(train_labels)\n",
    "#convert to float\n",
    "train = np.asarray(train).astype('float32')\n",
    "test = np.asarray(test).astype('float32')\n",
    "#reshape data for recurrent convolution\n",
    "rcnntrain = train\n",
    "rcnntest = test\n",
    "rcnntrain.shape = ((len(rcnntrain), max_len, 1))\n",
    "rcnntest.shape = (len(rcnntest), max_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7dcd426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define early stopping parameters\n",
    "es = EarlyStopping(monitor='val_loss', mode='min',\\\n",
    "                   patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a4888536",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(578)\n",
    "\n",
    "#define rcnn model\n",
    "\n",
    "cnn = models.Sequential()\n",
    "\n",
    "cnn.add(Embedding(max_words, 256, input_length = max_len))\n",
    "\n",
    "cnn.add(Conv1D(64, 3, padding = 'same', activation = 'relu',\\\n",
    "              kernel_regularizer = l2(1e-8)))\n",
    "cnn.add(BatchNormalization())\n",
    "\n",
    "cnn.add(Conv1D(64, 5, padding = 'same', activation = 'relu',\\\n",
    "              kernel_regularizer = l2(1e-8)))\n",
    "cnn.add(BatchNormalization())\n",
    "\n",
    "cnn.add(GlobalMaxPooling1D())\n",
    "\n",
    "cnn.add(Dense(64, activation = 'relu',\\\n",
    "              kernel_regularizer = l2(1e-8)))\n",
    "cnn.add(BatchNormalization())\n",
    "\n",
    "rcnn = models.Sequential()\n",
    "rcnn.add(TimeDistributed(cnn))\n",
    "rcnn.add(Dropout(0.5))\n",
    "rcnn.add(Bidirectional(GRU(128,\n",
    "            dropout = 0.5,\n",
    "            recurrent_dropout = 0.5,\n",
    "            kernel_regularizer = l2(1e-8))))\n",
    "rcnn.add(BatchNormalization())\n",
    "rcnn.add(Dense(128, activation = 'relu',\n",
    "            kernel_regularizer = l2(1e-8)))\n",
    "rcnn.add(BatchNormalization())\n",
    "rcnn.add(Dropout(0.5))\n",
    "\n",
    "rcnn.add(Dense(7, activation = 'softmax'))\n",
    "\n",
    "#compile model\n",
    "rcnn.compile(optimizer = 'rmsprop',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "history_rcnn = rcnn.fit(rcnntrain, train_labels,\n",
    "                       epochs = 100,\n",
    "                       batch_size = 64,\n",
    "                       validation_split = 0.2,\n",
    "                       callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rcnn.predict(test)\n",
    "preds = np.array([np.argmax(x) for x in preds])\n",
    "out = pd.DataFrame(np.transpose([test_id, preds]),\\\n",
    "                  columns = ['Id', 'Cat_code'])\n",
    "out.to_csv('predictions.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
